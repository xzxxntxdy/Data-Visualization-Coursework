import json
import os
import shutil
from collections import defaultdict
import heapq

# ================= é…ç½®è·¯å¾„ =================
# ä½ çš„æœ¬åœ°å›¾ç‰‡è·¯å¾„
COCO_IMAGE_DIR = r"D:\vlmdata\COCO2017\train2017"

# ä½ çš„æ ‡æ³¨æ–‡ä»¶è·¯å¾„
ANN_DIR = "src/data"
INSTANCES_PATH = os.path.join(ANN_DIR, "instances_train2017.json")
KEYPOINTS_PATH = os.path.join(ANN_DIR, "person_keypoints_train2017.json")
CAPTIONS_PATH = os.path.join(ANN_DIR, "captions_train2017.json")

# è¾“å‡ºè·¯å¾„
OUTPUT_IMAGE_PATH = os.path.join(ANN_DIR, "hero_image.jpg")
OUTPUT_JSON_PATH = os.path.join(ANN_DIR, "hero_data.json")

# ================= ç­›é€‰æ ‡å‡† =================
# æˆ‘ä»¬å¯»æ‰¾ä¸€å¼ "è®²æ•…äº‹"çš„å®Œç¾å›¾ç‰‡
TARGET_PERSON_COUNT = (3, 6)  # äººæ•°åœ¨ 3 åˆ° 6 ä¹‹é—´
MIN_TOTAL_OBJECTS = 8         # è‡³å°‘æœ‰ 8 ä¸ªç‰©ä½“ï¼ˆçœ‹èµ·æ¥ä¸°å¯Œï¼‰
MIN_UNIQUE_CATEGORIES = 4     # è‡³å°‘æœ‰ 4 ç§ä¸åŒç±»åˆ«çš„ç‰©ä½“ï¼ˆè¯­ä¹‰ä¸°å¯Œï¼‰

print("ğŸš€ å¼€å§‹åŠ è½½ COCO æ ‡æ³¨æ–‡ä»¶... (è¿™å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ)")

# 1. åŠ è½½ Instances
with open(INSTANCES_PATH, 'r') as f:
    instances_data = json.load(f)

# å»ºç«‹ç±»åˆ« ID åˆ° åç§° çš„æ˜ å°„
cat_id_to_name = {cat['id']: cat['name'] for cat in instances_data['categories']}

# 2. é¢„å¤„ç†ï¼šæŒ‰ Image ID ç»„ç»‡æ ‡æ³¨
img_anns = defaultdict(list)
for ann in instances_data['annotations']:
    img_anns[ann['image_id']].append(ann)

print(f"âœ… Instances åŠ è½½å®Œæ¯•ï¼Œå…± {len(img_anns)} å¼ æœ‰æ ‡æ³¨çš„å›¾ç‰‡ã€‚å¼€å§‹ç­›é€‰å€™é€‰è€…...")

# 3. ç­›é€‰ç®—æ³•
candidates = []

for img_id, anns in img_anns.items():
    # ç»Ÿè®¡ä¿¡æ¯
    person_count = 0
    categories = set()
    bbox_areas = [] # ç”¨äºæ£€æŸ¥å°ºåº¦å¤šæ ·æ€§
    
    for ann in anns:
        cat_name = cat_id_to_name.get(ann['category_id'])
        categories.add(cat_name)
        bbox_areas.append(ann['bbox'][2] * ann['bbox'][3]) # w * h
        if cat_name == 'person':
            person_count += 1
            
    # --- ç­›é€‰æ¡ä»¶ ---
    # æ¡ä»¶1: äººæ•°åˆé€‚ (ä¸ºäº†å±•ç¤º Pose)
    if not (TARGET_PERSON_COUNT[0] <= person_count <= TARGET_PERSON_COUNT[1]):
        continue
        
    # æ¡ä»¶2: è¯­ä¹‰ä¸°å¯Œ (ä¸ºäº†å±•ç¤º Semantic)
    if len(categories) < MIN_UNIQUE_CATEGORIES:
        continue
        
    # æ¡ä»¶3: ç‰©ä½“æ€»æ•° (ä¸ºäº†å±•ç¤º Spatial è¿™ç§å¯†é›†æ„Ÿ)
    if len(anns) < MIN_TOTAL_OBJECTS:
        continue
        
    # æ¡ä»¶4: å°ºåº¦å¤šæ ·æ€§ (Small < 32^2, Large > 96^2)
    has_small = any(area < 32*32 for area in bbox_areas)
    has_large = any(area > 96*96 for area in bbox_areas)
    
    if not (has_small and has_large):
        continue

    # å¦‚æœé€šè¿‡æ‰€æœ‰ç­›é€‰ï¼Œè®¡ç®—ä¸€ä¸ªâ€œå®Œç¾åˆ†æ•°â€
    # åˆ†æ•° = ç‰©ä½“æ•°é‡ + ç±»åˆ«æ•°é‡ * 2 (æˆ‘ä»¬æ›´çœ‹é‡ç±»åˆ«ä¸°å¯Œåº¦)
    score = len(anns) + len(categories) * 2
    candidates.append((score, img_id, anns))

# å–åˆ†æ•°æœ€é«˜çš„ Top 1
if not candidates:
    print("âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å®Œç¾å›¾ç‰‡ï¼Œè¯·æ”¾å®½ç­›é€‰æ ‡å‡†ã€‚")
    exit()

# æŒ‰åˆ†æ•°æ’åºï¼Œå–æœ€å¥½çš„ä¸€ä¸ª
best_candidate = sorted(candidates, key=lambda x: x[0], reverse=True)[0]
score, best_img_id, best_anns = best_candidate

print(f"ğŸ‰ æ‰¾åˆ°æœ€ä½³ Hero Image! ID: {best_img_id} (å¾—åˆ†: {score})")

# ================= è·å–è¯¦ç»†æ•°æ® =================

# 4. è·å–å¯¹åº”çš„å›¾ç‰‡æ–‡ä»¶å
# åœ¨ instances_data['images'] ä¸­æŸ¥æ‰¾æ–‡ä»¶å
file_name = next((img['file_name'] for img in instances_data['images'] if img['id'] == best_img_id), None)
if not file_name:
    print(f"âŒ æ‰¾ä¸åˆ° ID {best_img_id} å¯¹åº”çš„æ–‡ä»¶å")
    exit()

src_img_path = os.path.join(COCO_IMAGE_DIR, file_name)
if not os.path.exists(src_img_path):
    print(f"âŒ æœ¬åœ°å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {src_img_path}")
    print("è¯·æ£€æŸ¥ COCO_IMAGE_DIR è·¯å¾„é…ç½®æ˜¯å¦æ­£ç¡®ã€‚")
    exit()

# 5. åŠ è½½ Keypoints (ä»…é’ˆå¯¹è¿™å¼ å›¾)
print("æ­£åœ¨æå–å§¿æ€æ•°æ®...")
with open(KEYPOINTS_PATH, 'r') as f:
    kp_data = json.load(f)
    
# æ‰¾åˆ°å¯¹åº” image_id çš„ keypoints annotation
hero_keypoints = [ann for ann in kp_data['annotations'] if ann['image_id'] == best_img_id]

# 6. åŠ è½½ Captions (ä»…é’ˆå¯¹è¿™å¼ å›¾)
print("æ­£åœ¨æå–æè¿°æ•°æ®...")
with open(CAPTIONS_PATH, 'r') as f:
    cap_data = json.load(f)

hero_captions = [ann['caption'] for ann in cap_data['annotations'] if ann['image_id'] == best_img_id]

# ================= ç”Ÿæˆæœ€ç»ˆæ•°æ®ç»“æ„ =================

# å¤„ç† bbox æ•°æ®ï¼Œå¢åŠ  scale æ ‡ç­¾
processed_objects = []
for ann in best_anns:
    area = ann['bbox'][2] * ann['bbox'][3]
    scale = "medium"
    if area < 32 * 32: scale = "small"
    elif area > 96 * 96: scale = "large"
    
    processed_objects.append({
        "id": ann['id'],
        "category": cat_id_to_name[ann['category_id']],
        "bbox": ann['bbox'], # [x, y, w, h]
        "area": area,
        "scale": scale,
        "iscrowd": ann['iscrowd']
    })

# ç®€åŒ– keypoints æ•°æ®
processed_poses = []
for ann in hero_keypoints:
    # COCO keypoints æ ¼å¼: [x1, y1, v1, x2, y2, v2, ...]
    # v=0: not labeled, v=1: labeled but not visible, v=2: labeled and visible
    if ann['num_keypoints'] > 0:
        processed_poses.append({
            "id": ann['id'],
            "keypoints": ann['keypoints'],
            "bbox": ann['bbox'] # äººä½“çš„æ¡†ï¼Œç”¨äºå¯¹é½
        })

hero_data = {
    "meta": {
        "image_id": best_img_id,
        "file_name": file_name,
        "captions": hero_captions
    },
    "spatial": processed_objects, # ç”¨äºåœºæ™¯ä¸€
    "semantic": {
        # ç®€å•æ„å»ºä¸€ä¸ªå…±ç°åˆ—è¡¨ï¼Œå®é™…å‰ç«¯å¯è§†åŒ–æ—¶å¯ä»¥åªè¿æ¥è¿™äº›ç‰©ä½“
        "categories": list(set(obj['category'] for obj in processed_objects))
    },
    "pose": processed_poses # ç”¨äºåœºæ™¯ä¸‰
}

# ================= å†™å…¥æ–‡ä»¶ =================

# 1. å¤åˆ¶å›¾ç‰‡
print(f"æ­£åœ¨å¤åˆ¶å›¾ç‰‡: {file_name} -> hero_image.jpg")
shutil.copy2(src_img_path, OUTPUT_IMAGE_PATH)

# 2. å†™å…¥ JSON
print("æ­£åœ¨ä¿å­˜ hero_data.json...")
with open(OUTPUT_JSON_PATH, 'w', encoding='utf-8') as f:
    json.dump(hero_data, f, indent=2, ensure_ascii=False)

print("âœ… å…¨éƒ¨å®Œæˆï¼")
print(f"å›¾ç‰‡ä½ç½®: {OUTPUT_IMAGE_PATH}")
print(f"æ•°æ®ä½ç½®: {OUTPUT_JSON_PATH}")